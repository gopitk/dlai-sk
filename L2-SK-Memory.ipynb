{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a786c77c",
   "metadata": {},
   "source": [
    "# SK: Memory\n",
    "\n",
    "## Outline\n",
    "* kernel.memory.save_information_async\n",
    "* VolatileMemoryStore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297dcd5",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f518f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f33c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "import os\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "kernel=sk.Kernel()\n",
    "api_key = os.environ['OPENAI_API_KEY']\n",
    "kernel.add_chat_service(\n",
    "        \"chat-gpt\", OpenAIChatCompletion(\"gpt-3.5-turbo\", api_key)\n",
    ")\n",
    "\n",
    "# $input is a reserved variable for skills/prompt templates with just one default variable\n",
    "chatty = kernel.create_semantic_function(\"{{ $input }}\", temperature=0.0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40e5de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat (instr) :\n",
    "    # use input_str to pass data into the semantic function $input variable\n",
    "    output = await kernel.run_async(chatty, input_str=instr)\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a114a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def populate_memory(kernel: sk.Kernel) -> None:\n",
    "    # Add some documents to the semantic memory\n",
    "    await kernel.memory.save_information_async(\n",
    "        \"aboutMe\", id=\"info1\", text=\"My name is Andrew\"\n",
    "    )\n",
    "    await kernel.memory.save_information_async(\n",
    "        \"aboutMe\", id=\"info2\", text=\"I currently work as a tour guide\"\n",
    "    )\n",
    "    await kernel.memory.save_information_async(\n",
    "        \"aboutMe\", id=\"info3\", text=\"I've been living in Seattle since 2005\"\n",
    "    )\n",
    "    await kernel.memory.save_information_async(\n",
    "        \"aboutMe\", id=\"info4\", text=\"I visited France and Italy five times since 2015\"\n",
    "    )\n",
    "    await kernel.memory.save_information_async(\n",
    "        \"aboutMe\", id=\"info5\", text=\"My family is from New York\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18730564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "async def setup_chat_with_memory(\n",
    "    kernel: sk.Kernel,\n",
    ") -> Tuple[sk.SKFunctionBase, sk.SKContext]:\n",
    "    sk_prompt = \"\"\"\n",
    "    ChatBot can have a conversation with you about any topic.\n",
    "    It can give explicit instructions or say 'I don't know' if\n",
    "    it does not have an answer.\n",
    "\n",
    "    Information about me, from previous conversations:\n",
    "    - {{$fact1}} {{recall $fact1}}\n",
    "    - {{$fact2}} {{recall $fact2}}\n",
    "    - {{$fact3}} {{recall $fact3}}\n",
    "    - {{$fact4}} {{recall $fact4}}\n",
    "    - {{$fact5}} {{recall $fact5}}\n",
    "\n",
    "    User: {{$user_input}}\n",
    "    \"\"\"\n",
    "\n",
    "    chat_func = kernel.create_semantic_function(\n",
    "        sk_prompt, max_tokens=200, temperature=0.0\n",
    "    )\n",
    "\n",
    "    context = kernel.create_new_context()\n",
    "    context[\"fact1\"] = \"what is my name?\"\n",
    "    context[\"fact2\"] = \"where do I live?\"\n",
    "    context[\"fact3\"] = \"where's my family from?\"\n",
    "    context[\"fact4\"] = \"where have I traveled?\"\n",
    "    context[\"fact5\"] = \"what do I do for work?\"\n",
    "\n",
    "    context[sk.core_skills.TextMemorySkill.COLLECTION_PARAM] = \"aboutMe\"\n",
    "    context[sk.core_skills.TextMemorySkill.RELEVANCE_PARAM] = 0.8\n",
    "\n",
    "    return chat_func, context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766fa1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Andrew! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "output=await chat(\"Hi, My name is Andrew!\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2ef90ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1 equals 2.\n"
     ]
    }
   ],
   "source": [
    "output=await chat(\"What is 1+1\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2c87f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have access to personal information about individuals unless it has been shared with me in the course of our conversation.\n"
     ]
    }
   ],
   "source": [
    "output=await chat(\"What is my name?\")\n",
    "print(output)\n",
    "# Without the memory LLM does not know previous info shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cd476e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextEmbedding\n",
    "kernel.add_text_embedding_generation_service(\n",
    "        \"ada\", OpenAITextEmbedding(\"text-embedding-ada-002\", api_key)\n",
    "    )\n",
    "\n",
    "# Give SK some memory to use with the LLM\n",
    "kernel.register_memory_store(memory_store=sk.memory.VolatileMemoryStore())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "311978ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': <semantic_kernel.orchestration.sk_function.SKFunction at 0x7fa808037370>,\n",
       " 'save': <semantic_kernel.orchestration.sk_function.SKFunction at 0x7fa808037490>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel.import_skill(sk.core_skills.TextMemorySkill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88bdf13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "await populate_memory(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97dd6e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up a chat (with memory!)\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up a chat (with memory!)\")\n",
    "chat_func, context = await setup_chat_with_memory(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "165b1662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot: 1+1 equals 2.\n"
     ]
    }
   ],
   "source": [
    "context[\"user_input\"] = \"What is 1+1?\" \n",
    "answer = await kernel.run_async(chat_func, input_vars=context.variables)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db24677d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, your name is Andrew.\n"
     ]
    }
   ],
   "source": [
    "context[\"user_input\"] = \"Can you tell my name?\" \n",
    "answer = await kernel.run_async(chat_func, input_vars=context.variables)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc3ef937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You mentioned that your family is from New York.\n"
     ]
    }
   ],
   "source": [
    "context[\"user_input\"] = \"Can you tell me where I hail from?\" \n",
    "answer = await kernel.run_async(chat_func, input_vars=context.variables)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acf3339a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You currently live in Seattle.\n"
     ]
    }
   ],
   "source": [
    "context[\"user_input\"] = \"Can you tell me where I live currently?\" \n",
    "answer = await kernel.run_async(chat_func, input_vars=context.variables)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98e9ff",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1bae7",
   "metadata": {},
   "source": [
    "### User managed in SK for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2931b92",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3029c580",
   "metadata": {},
   "source": [
    "### User managed in SK for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff55d5d",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13da00e0",
   "metadata": {},
   "source": [
    "### User managed in SK for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626af62e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
